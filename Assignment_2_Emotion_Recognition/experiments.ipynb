{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuwngCdvGxiHZZgpRDMMcF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathrin229/computer_vision/blob/main/Assignment_2_Emotion_Recognition/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG35vLHM4lf7"
      },
      "source": [
        "# **Connect to Drive and GitHub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvKre7Aerq0",
        "outputId": "6fb3ee68-521a-4b86-b75a-7e1ad7953b2a"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qjpo7rqa2d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e215074-a26e-4b4a-a856-95581d5e06f9"
      },
      "source": [
        "git_username = ''\n",
        "git_token =  ''\n",
        "\n",
        "if git_username == '':\n",
        "  print('Github username:')\n",
        "  git_username = %sx read -p ''\n",
        "  git_username = git_username[0]\n",
        "\n",
        "if git_token == '':\n",
        "  print('Github access token (https://github.com/settings/tokens):')\n",
        "  print('Github Token:')\n",
        "  git_token = %sx read -p ''\n",
        "  git_token = git_token[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Github username:\n",
            "Github access token (https://github.com/settings/tokens):\n",
            "Github Token:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIWvRvVMbYp9",
        "outputId": "682e0946-6195-42b7-d5ac-a6d4fb15aadf"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf computer_vision # To remove the previous clone of the Github repository\n",
        "!git clone -l -s https://$git_username:$git_token@github.com/kathrin229/computer_vision.git computer_vision\n",
        "%cd computer_vision\n",
        "%cd Assignment_2_Emotion_Recognition\n",
        "!ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'computer_vision'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 530, done.\u001b[K\n",
            "remote: Counting objects: 100% (530/530), done.\u001b[K\n",
            "remote: Compressing objects: 100% (379/379), done.\u001b[K\n",
            "remote: Total 530 (delta 242), reused 418 (delta 148), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (530/530), 19.75 MiB | 23.17 MiB/s, done.\n",
            "Resolving deltas: 100% (242/242), done.\n",
            "/content/computer_vision\n",
            "/content/computer_vision/Assignment_2_Emotion_Recognition\n",
            " dataset.py\t\t\t\t    models.py\n",
            "'Deep Learning assignment using CNNs.pdf'   plots.py\n",
            " experiments.ipynb\t\t\t   'PyTorch Short Tutorial.pdf'\n",
            " main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ0Av-8SK6NR"
      },
      "source": [
        "# **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL15ELoKK44Y"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import dataset\n",
        "from models import Conv1DNet1Layer, Conv1DNet2Layer, Conv2DNet1Layer, Conv2DNet2Layer, Conv2DNet3Layer, Conv2DNet4Layer, Conv2DNet5Layer, Conv2DNet6Layer, Conv2DNet7Layer, Conv2DNet8Layer, Conv2DNet9Layer, Conv2DNet10Layer\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import plots\n",
        "\n",
        "######################################################\n",
        "# Configuration\n",
        "######################################################\n",
        "\n",
        "architecture = Conv1DNet1Layer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "patience = 20\n",
        "# model_args = {\n",
        "#     'input_channel': 1,\n",
        "\n",
        "#     'channel_layer1': 32,\n",
        "#     'kernel_layer1': 5,\n",
        "#     'stride_layer1': 2,\n",
        "#     'padding_layer1': 2,\n",
        "\n",
        "#     'channel_layer2': 64,\n",
        "#     'kernel_layer2': 5,\n",
        "#     'stride_layer2': 2,\n",
        "#     'padding_layer2': 2,\n",
        "\n",
        "#     'channel_layer3': 0,\n",
        "#     'kernel_layer3': 0,\n",
        "#     'stride_layer3': 0,\n",
        "#     'padding_layer3': 0,\n",
        "\n",
        "#     'channel_layer4': 0,\n",
        "#     'kernel_layer4': 0,\n",
        "#     'stride_layer4': 0,\n",
        "#     'padding_layer4': 0,\n",
        "\n",
        "#     'channel_layer5': 0,\n",
        "#     'kernel_layer5': 0,\n",
        "#     'stride_layer5': 0,\n",
        "#     'padding_layer5': 0,\n",
        "\n",
        "#     'channel_layer6': 0,\n",
        "#     'kernel_layer6': 0,\n",
        "#     'stride_layer6': 0,\n",
        "#     'padding_layer6': 0,\n",
        "\n",
        "#     'channel_layer7': 0,\n",
        "#     'kernel_layer7': 0,\n",
        "#     'stride_layer7': 0,\n",
        "#     'padding_layer7': 0,\n",
        "\n",
        "#     'channel_layer8': 0,\n",
        "#     'kernel_layer8': 0,\n",
        "#     'stride_layer8': 0,\n",
        "#     'padding_layer8': 0,\n",
        "\n",
        "#     'channel_layer9': 0,\n",
        "#     'kernel_layer9': 0,\n",
        "#     'stride_layer9': 0,\n",
        "#     'padding_layer9': 0,\n",
        "\n",
        "#     'channel_layer10': 0,\n",
        "#     'kernel_layer10': 0,\n",
        "#     'stride_layer10': 0,\n",
        "#     'padding_layer10': 0,\n",
        "\n",
        "#     'channel_linear': 6*6*64,\n",
        "#     'num_classes': 7\n",
        "# }\n",
        "\n",
        "######################################################"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFBNkp3uyv4C"
      },
      "source": [
        "# **Data Loading**\n",
        "Make sure to set DATA_DIR to path where the dataset (data.npy) is stored in Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttcHF81la5Iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c81ecc-3ebe-4fde-8f7e-0705843d0269"
      },
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/CV/data\"\n",
        "dataset_path = os.path.join(DATA_DIR, 'data.npy')\n",
        "IMG_DIR = \"/content/drive/MyDrive/CV/img/\"\n",
        "\n",
        "data = dataset.load_data(src='data/fer2013/fer2013/fer2013.csv', dest=dataset_path)\n",
        "train_loader, valid_loader, test_loader = dataset.get_data_loader(data, batch_size, architecture=architecture,\n",
        "                                                                  shuffle=True, drop_last=True)\n",
        "print(\"Finished loading data.\\n\")\n",
        "\n",
        "classes = {\n",
        "    0: 'Angry',\n",
        "    1: 'Disgust',\n",
        "    2: 'Fear',\n",
        "    3: 'Happy',\n",
        "    4: 'Sad',\n",
        "    5: 'Surprise',\n",
        "    6: 'Neutral'\n",
        "}"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "Finished loading data.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uqMW81Iy5yQ"
      },
      "source": [
        "# **Train CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hpgQ_n5yXTt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "27863c3f-2b5c-4078-ba7d-81f592c77e38"
      },
      "source": [
        "# connect to GPU if one is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    print('Current device:', torch.cuda.get_device_name(device), '\\n')\n",
        "else:\n",
        "    print('Failed to find GPU. Will use CPU.\\n')\n",
        "    device = 'cpu'\n",
        "\n",
        "# create model\n",
        "#model = architecture(**model_args).to(device)\n",
        "model = architecture().to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(model)\n",
        "print(\"\\n\")\n",
        "\n",
        "# set seed for reproducibility\n",
        "torch.seed()\n",
        "torch.manual_seed(0)\n",
        "\n",
        "min_valid_loss = np.inf\n",
        "stopping = 0\n",
        "max_val_acc = 0\n",
        "train_loss_all = []\n",
        "train_acc_all = []\n",
        "valid_loss_all = []\n",
        "valid_acc_all = []\n",
        "\n",
        "print(\"Fit model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_correct, train_total = 0, 0, 0\n",
        "    train_epoch_loss = []\n",
        "    train_acc_epoch = []\n",
        "    ######################################################\n",
        "    # training loop (iterates over training batches)\n",
        "    ######################################################\n",
        "    for batch in train_loader:\n",
        "        x_train = batch[0].to(device)\n",
        "        y_train = batch[1].to(device)\n",
        "        # clear the old gradients from optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: feed inputs to the model to get outputs\n",
        "        y_pred = model(x_train.float())\n",
        "        # calculate the training batch loss\n",
        "        loss_train = loss(y_pred, y_train)\n",
        "        # backward: perform gradient descent of the loss w.r. to the model params\n",
        "        loss_train.backward()\n",
        "        # update the model parameters by performing a single optimization step\n",
        "        optimizer.step()\n",
        "        # accumulate the training loss\n",
        "        train_loss += loss_train.item()\n",
        "        train_epoch_loss.append(loss_train.item())\n",
        "        # calculate the accuracy\n",
        "        predicted = torch.argmax(y_pred, 1)\n",
        "        train_total += y_train.size(0)\n",
        "        train_correct += (predicted == y_train).sum().item()\n",
        "        train_acc_epoch.append((predicted == y_train).sum().item())\n",
        "\n",
        "    train_loss_all.append(sum(train_epoch_loss) / len(train_epoch_loss))\n",
        "    train_acc_all.append(sum(train_acc_epoch) / len(train_acc_epoch))\n",
        "\n",
        "    ######################################################\n",
        "    # validation loop\n",
        "    ######################################################\n",
        "    # set the model to eval mode\n",
        "    model.eval()\n",
        "    valid_loss, valid_correct, valid_total = 0, 0, 0\n",
        "    valid_epoch_loss = []\n",
        "    valid_acc_epoch = []\n",
        "    # turn off gradients for validation\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            x_valid = batch[0].to(device)\n",
        "            y_valid = batch[1].to(device)\n",
        "            # forward pass\n",
        "            y_pred = model(x_valid.float())\n",
        "            # validation batch loss\n",
        "            loss_valid = loss(y_pred, y_valid)\n",
        "            # accumulate the validation loss\n",
        "            valid_loss += loss_valid.item()\n",
        "            valid_epoch_loss.append(loss_valid.item())\n",
        "            # calculate the accuracy\n",
        "            predicted = torch.argmax(y_pred, 1)\n",
        "            valid_total += y_valid.size(0)\n",
        "            valid_correct += (predicted == y_valid).sum().item()\n",
        "            valid_acc_epoch.append((predicted == y_valid).sum().item())\n",
        "\n",
        "    valid_loss_all.append(sum(valid_epoch_loss) / len(valid_epoch_loss))\n",
        "    valid_acc_all.append(sum(valid_acc_epoch) / len(valid_acc_epoch))\n",
        "\n",
        "    # print epoch results\n",
        "    train_loss /= train_total  # len(train_loader)\n",
        "    valid_loss /= valid_total  # len(valid_loader)\n",
        "    train_accuracy = train_correct / train_total  # len(train_loader)\n",
        "    valid_accuracy = valid_correct / valid_total  # len(valid_loader)\n",
        "    print(f'Epoch: {epoch + 1}/{num_epochs}.. '\n",
        "          f'Training loss: {train_loss}.. Validation Loss: {valid_loss}.. '\n",
        "          f'Training accuracy: {train_accuracy}.. Validation accuracy: {valid_accuracy}')\n",
        "\n",
        "    # # early stopping (based on validation accuracy)\n",
        "    # if max_val_acc < valid_accuracy:\n",
        "    #     max_val_acc = valid_accuracy\n",
        "    #     weights = copy.deepcopy(model.state_dict())\n",
        "    #     stopping = 0\n",
        "    # else:\n",
        "    #     stopping = stopping + 1\n",
        "    #\n",
        "    # if stopping == patience:\n",
        "    #     print('Early stopping...')\n",
        "    #     print('Restoring best weights')\n",
        "    #     model.load_state_dict(weights)\n",
        "    #     break\n",
        "\n",
        "    # early stopping (based on validation loss)\n",
        "    if min_valid_loss > valid_loss:\n",
        "        min_valid_loss = valid_loss\n",
        "        weights = copy.deepcopy(model.state_dict())\n",
        "        stopping = 0\n",
        "    else:\n",
        "        stopping = stopping + 1\n",
        "\n",
        "    if stopping == patience:\n",
        "        print('Early stopping...')\n",
        "        print('Restoring best weights')\n",
        "        model.load_state_dict(weights)\n",
        "        break\n",
        "\n",
        "# plotting training and validation loss\n",
        "plots.plot_train_val(np.linspace(1, epoch + 1, epoch + 1).astype(int),\n",
        "                     train_loss_all, valid_loss_all,\n",
        "                     metric=\"Cross Entropy\", IMG_DIR=f'{model.__class__.__name__}')\n",
        "plots.plot_train_val(np.linspace(1, epoch + 1, epoch + 1).astype(int),\n",
        "                     train_acc_all, valid_acc_all,\n",
        "                     metric=\"Accuracy\", IMG_DIR=f'{model.__class__.__name__}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current device: Tesla T4 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d475794b7d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#model = architecture(**model_args).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 11 required positional arguments: 'input_channel', 'channel_layer1', 'kernel_layer1', 'stride_layer1', 'padding_layer1', 'channel_layer2', 'kernel_layer2', 'stride_layer2', 'padding_layer2', 'channel_linear', and 'num_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqUUgbKz2Wj"
      },
      "source": [
        "# **Test CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54e3pLrTzwZn"
      },
      "source": [
        "######################################################\n",
        "# test loop\n",
        "######################################################\n",
        "print(\"Test model...\")\n",
        "# set the model to eval mode\n",
        "model.eval()\n",
        "# turn off gradients for validation\n",
        "with torch.no_grad():\n",
        "    test_loss, test_correct, test_total = 0, 0, 0\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        x_test = batch[0].to(device)\n",
        "        y_test = batch[1].to(device)\n",
        "        # forward pass\n",
        "        y_pred = model(x_test.float())\n",
        "        # test batch loss\n",
        "        loss_test = loss(y_pred, y_test)\n",
        "        # accumulate the test loss\n",
        "        test_loss += loss_test.item()\n",
        "        # calculate the accuracy\n",
        "        predicted = torch.argmax(y_pred, 1)\n",
        "        test_total += y_test.size(0)\n",
        "        test_correct += (predicted == y_test).sum().item()\n",
        "        if i == 0:\n",
        "            # plot predictions for first 8 images in first batch\n",
        "            plots.plot_predictions(x_test.cpu(), y_test.cpu(), predicted.cpu(), classes, 15,\n",
        "                                   filename=IMG_DIR + f'{model.__class__.__name__}_predictions.png')\n",
        "\n",
        "test_loss /= test_total  # len(test_loader)\n",
        "accuracy = test_correct / test_total  # test_correct / len(test_loader)\n",
        "print(f'Test loss: {loss_test}.. Test accuracy: {accuracy}')\n",
        "\n",
        "precision, recall, fscore, support = precision_recall_fscore_support(y_test.cpu(), predicted.cpu(), average='macro')\n",
        "print(f'Precision (macro): {precision}.. Recall (macro): {recall}.. F-score (macro): {fscore}')\n",
        "\n",
        "# plot confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test.cpu(), predicted.cpu())\n",
        "fig = plots.print_confusion_matrix(cf_matrix, class_names=[classes[c] for c in np.unique(y_test.cpu())],\n",
        "                                   filename=IMG_DIR + f'{model.__class__.__name__}_cf_matrix.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzkdMa2z7c6"
      },
      "source": [
        "# **Visualize Feature Maps**\n",
        "Set IMG_DIR to path where feature maps are to be stored in Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtsaWpnZzqEm"
      },
      "source": [
        "\n",
        "######################################################\n",
        "# visualization of feature maps for single image\n",
        "######################################################\n",
        "# reference: https://androidkt.com/how-to-visualize-feature-maps-in-convolutional-neural-networks-using-pytorch/\n",
        "\n",
        "img = x_train[0].unsqueeze(0).type(torch.FloatTensor).to(device)\n",
        "\n",
        "# accessing convolutional layers\n",
        "num_layers = 0\n",
        "conv_layers = []\n",
        "model_children = list(model.children())\n",
        "\n",
        "for child in model_children:\n",
        "    if type(child) == nn.Conv2d:\n",
        "        num_layers += 1\n",
        "        conv_layers.append(child)\n",
        "    elif type(child) == nn.Sequential:\n",
        "        for layer in child.children():\n",
        "            if type(layer) == nn.Conv2d:\n",
        "                num_layers += 1\n",
        "                conv_layers.append(layer)\n",
        "\n",
        "# pass image through network and store results\n",
        "results = [conv_layers[0](img)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "outputs = results\n",
        "\n",
        "# plot image\n",
        "plt.imshow(x_train[0].cpu().flatten().reshape(48, 48), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# visualize feature maps of network\n",
        "for num_layer in range(len(outputs)):\n",
        "    fig = plt.figure(figsize=(50, 10))\n",
        "    layer_viz = outputs[num_layer][0, :, :, :]\n",
        "    layer_viz = layer_viz.data\n",
        "    title = \"Layer %s\" % (num_layer + 1)\n",
        "    print(title)\n",
        "    for i, conv_filter in enumerate(layer_viz):\n",
        "        if i == 16:\n",
        "            break\n",
        "        plt.subplot(2, 8, i + 1)\n",
        "        plt.imshow(conv_filter.cpu(), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "        st = fig.suptitle(title, fontsize=50)\n",
        "        # shift subplots down:\n",
        "        st.set_y(0.95)\n",
        "        fig.subplots_adjust(top=0.85)\n",
        "    plt.savefig(IMG_DIR + model.__class__.__name__ + \"_layer%s_feature_maps.png\" % str(num_layer + 1))\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvDL5MKQFaWt"
      },
      "source": [
        "print(\"train_loss,          valid_loss,         train_accuracy,    valid_accuracy,     accuracy,          precision,          recall,             fscore\")\n",
        "print(train_loss, valid_loss, train_accuracy, valid_accuracy, accuracy, precision, recall, fscore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}